---
title: Guía de soluciones de modelado y análisis de riesgos actuariales
author: scseely
ms.author: scseely
ms.date: 08/23/2018
ms.topic: article
ms.service: industry
description: En esta guía de soluciones se explica cómo un desarrollador actuarial puede trasladar su solución existente más la infraestructura auxiliar a Azure.
ms.openlocfilehash: 82cb53d529f6d7524ae1f9c148118b5edddc648b
ms.sourcegitcommit: 76f2862adbec59311b5888e043a120f89dc862af
ms.translationtype: HT
ms.contentlocale: es-ES
ms.lasthandoff: 11/03/2018
ms.locfileid: "51654292"
---
# <a name="actuarial-risk-analysis-and-financial-modeling-solution-guide"></a>Guía de soluciones de modelización financiera y análisis de riesgos actuariales

En los últimos años, las aseguradoras y las compañías que ofrecen productos similares a los seguros han visto varias regulaciones nuevas. Estas nuevas regulaciones han necesitado un modelado financiero más amplio para las aseguradoras. La Unión Europea ha aprobado [Solvencia II](https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=celex%3A32009L0138), que requiere que las aseguradoras demuestren que han realizado un análisis adecuado para validar que serán solventes al final del año. Las aseguradoras que proporcionan anualidades variables deben seguir la [Guía actuarial XLIII](https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=celex%3A32009L0138) con un análisis extenso de los flujos de caja de activos y pasivos. Todos los tipos de aseguradoras, incluidas aquellas que distribuyen seguros como productos, tendrá que implementar las [Normas Internacionales de Información Financiera 17](https://www.ifrs.org/supporting-implementation/supporting-materials-by-ifrs-standard/ifrs-17/) (NIIF 17) para 2021. Existen otras normativas según las jurisdicciones en que las que operan las aseguradoras. Estos estándares y regulaciones requieren que los actuarios utilicen técnicas de cálculo intensivo al modelar activos y pasivos. Gran parte del análisis hará uso de datos de escenario generados de forma estocástica sobre aportaciones en serie de cosas como activos y pasivos. Más allá de las necesidades regulatorias, los actuarios realizan una buena cantidad de modelos y cálculos financieros para generar las tablas de entrada para los modelos que generan los informes regulatorios. Las cuadrículas internas no satisfacen las necesidades de cálculo, por lo que los actuarios se están trasladando a la nube constantemente.

Los actuarios se mueven a la nube para tener más tiempo para revisar, evaluar y validar los resultados. Cuando los reguladores auditan a las aseguradoras, los actuarios deben poder explicar sus resultados. El traslado a la nube proporciona acceso a los recursos de cálculo para ejecutar 20 000 horas de análisis en 24-120 horas de tiempo gracias al potencial de la paralelización. Para ayudar con esta necesidad de escala, muchas de las compañías que crean software actuarial proporcionan soluciones que permiten que los cálculos se ejecuten en Azure. Algunas de estas soluciones se basan en tecnologías que se ejecutan en local y Azure, como [HPC Pack](https://docs.microsoft.com/powershell/high-performance-computing/overview?view=hpc16-ps&WT.mc_id=riskmodel-docs-scseely). Otras son nativas de Azure y usan [Azure Batch](https://docs.microsoft.com/azure/batch?WT.mc_id=riskmodel-docs-scseely), [Virtual Machine Scale Sets](https://docs.microsoft.com/azure/virtual-machine-scale-sets?WT.mc_id=riskmodel-docs-scseely) o una solución de escalado personalizada.

En este artículo, veremos cómo los desarrolladores actuariales pueden usar Azure, junto con los paquetes de modelado, para analizar el riesgo. En el artículo se explican algunas de las tecnologías de Azure que los paquetes de modelado utilizan para ejecutarse a escala en Azure. Puede usar la misma tecnología para realizar más análisis de los datos. Fijémonos en los siguientes elementos:

- Ejecución de modelos más grandes en menos tiempo, en Azure.
- Creación de informes sobre los resultados.
- Administración de retención de datos.

Ya sea que esté prestando servicios de seguros de vida, propiedad y accidentes, salud u otro tipo de seguros, debe crear modelos financieros y de riesgo de sus activos y pasivos para ajustar sus inversiones y primas de forma que sea solvente como asegurador. Los informes de NIIF 17 agregan cambios a los modelos que los actuarios crean, como el cálculo del margen de servicio contractual (CSM), que cambia la forma en que las aseguradoras administran sus ganancias a lo largo del tiempo.

## <a name="running-more-in-less-time-in-azure"></a>Ejecutar más en menos tiempo, en Azure

Cree en la promesa de la nube: puede ejecutar sus modelos financieros y de riesgo de manera más rápida y sencilla. Para muchas aseguradoras, un cálculo de servilleta muestra el problema: necesitan años, o incluso décadas, de tiempo secuencial para ejecutar estos cálculos de principio a fin. Necesita la tecnología para resolver el problema en tiempo de ejecución. Las estrategias son:

- Preparación de datos: algunos datos cambian lentamente. Una vez que una póliza o contrato de servicio entra en vigor, las reclamaciones se mueven a un ritmo predecible. Puede preparar los datos necesarios para las ejecuciones de modelo a medida que llegan, lo que elimina la necesidad de planear mucho tiempo de preparación y limpieza de datos. También puede usar la agrupación en clústeres para crear sustituciones para datos en serie a través de representaciones ponderadas. Por lo general, menos registros dan lugar a un tiempo de cálculo reducido.
- Paralelización: si tiene que realizar el mismo análisis para dos o más elementos, puede llevar a cabo el análisis al mismo tiempo.

Echemos un vistazo a estos elementos individualmente.

### <a name="data-preparation"></a>Preparación de los datos

Los datos fluyen desde varios orígenes diferentes. Tiene datos de pólizas semiestructurados en sus libros de negocios. Dispone de información sobre los asegurados, las compañías y los elementos que aparecen en varios formularios de solicitud. Los generadores de escenarios económicos (ESG) producen datos en diversos formatos que pueden requerir la conversión a un formulario que su modelo pueda usar. Los datos actuales sobre valores de activos también necesitan normalización. Los datos del mercado de valores, los datos del flujo de caja de los alquileres, la información de pago de las hipotecas y otros datos de activos necesitan cierta preparación al pasar del origen al modelo. Finalmente, debe actualizar todas las suposiciones basándose en los datos de las experiencias recientes. Para acelerar la ejecución de un modelo, prepare los datos con anticipación. En el momento del tiempo de ejecución, realiza todas las actualizaciones necesarias para agregar cambios desde la última actualización programada.

Por lo tanto, ¿cómo prepara los datos? Primero veamos las partes comunes y luego veamos cómo trabajar con las diferentes formas en que aparecerán los datos. En primer lugar, desea un mecanismo para obtener todos los cambios desde la última sincronización. Ese mecanismo debe incluir un valor que sea clasificable. Para cambios recientes, ese valor debe ser mayor que cualquier cambio anterior. Los dos mecanismos más comunes son un campo de identificador cada vez mayor o una marca de tiempo. Si un registro tiene una clave de identificador creciente, pero el resto del registro contiene campos que se pueden actualizar, debe usar algo como una marca de tiempo de &quot;última modificación&quot; para encontrar cambios. Una vez que se hayan procesado los registros, registre el valor clasificable del último artículo actualizado. Este valor, probablemente una marca de tiempo en un campo llamado _lastModified_, se convierte en su marca de agua, que se utiliza para consultas posteriores en el almacén de datos. Los cambios de datos se pueden procesar de muchas maneras. Aquí hay dos mecanismos comunes que utilizan recursos mínimos:

1. Si tiene cientos o miles de cambios para procesar: cargue los datos en el almacenamiento de blobs. Use un desencadenador de eventos en [Azure Data Factory](https://docs.microsoft.com/azure/data-factory?WT.mc_id=riskmodel-docs-scseely) para procesar el conjunto de cambios.
2. Si tiene pequeños conjuntos de cambios para procesar o desea actualizar sus datos tan pronto como ocurra un cambio, coloque cada cambio en un mensaje de cola hospedado por [Service Bus](https://docs.microsoft.com/azure/service-bus-messaging?WT.mc_id=riskmodel-docs-scseely) o [colas de almacenamiento](https://docs.microsoft.com/azure/storage/queues/storage-queues-introduction?WT.mc_id=riskmodel-docs-scseely). [Este artículo](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-azure-and-service-bus-queues-compared-contrasted?WT.mc_id=riskmodel-docs-scseely) contiene una excelente explicación sobre el equilibrio entre las dos tecnologías de colas. Una vez que un mensaje está en una cola, puede usar un desencadenador en Azure Functions o Azure Data Factory para procesar dicho mensaje.

En la siguiente ilustración se muestra un escenario típico. En primer lugar, un trabajo programado recopila un conjunto de datos y coloca el archivo en el almacenamiento. El trabajo programado puede ser un trabajo CRON que se ejecuta en local, una [tarea de Scheduler](https://docs.microsoft.com/azure/scheduler?WT.mc_id=riskmodel-docs-scseely), una [aplicación lógica](https://docs.microsoft.com/azure/logic-apps/logic-apps-overview?WT.mc_id=riskmodel-docs-scseely) o cualquier cosa que se ejecute en un temporizador. Una vez que se carga el archivo, se puede desencadenar una instancia de [Azure Functions](https://docs.microsoft.com/azure/azure-functions?WT.mc_id=riskmodel-docs-scseely) o **Data Factory** para procesar los datos. Si el archivo se puede procesar en un corto período de tiempo, use una **función**. Si el procesamiento es complejo, requiere inteligencia artificial u otros scripts complejos, es posible que [HDInsight](https://docs.microsoft.com/azure/hdinsight?WT.mc_id=riskmodel-docs-scseely), [Azure Databricks](https://docs.microsoft.com/azure/azure-databricks?WT.mc_id=riskmodel-docs-scseely) o algo personalizado funcione mejor. Cuando finalice, el archivo acaba en una forma utilizable como un archivo nuevo o como registros en una base de datos.

 ![](./assets/insurance-risk-assets/process-files.png)

Una vez que los datos están en Azure, necesita que la aplicación de modelado los pueda usar. Puede escribir código para realizar transformaciones personalizadas, ejecutar los elementos a través de **HDInsight** o Azure **Databricks** para ingerir elementos más grandes o copiar los datos en los conjuntos de datos correctos. El uso de herramientas de macrodatos también puede ayudarle a hacer cosas como transformar datos no estructurados en datos estructurados, así como ejecutar cualquier IA y ML sobre los datos recibidos. También puede hospedar máquinas virtuales, cargar datos directamente en orígenes desde el entorno local, llamar directamente a Azure Functions, etc.

Más adelante, los modelos deben consumir los datos. La forma de hacerlo depende en gran medida de cómo los cálculos necesitan acceder a los datos. Algunos sistemas de modelado requieren que todos los archivos de datos se encuentren en el nodo que ejecuta el cálculo. Otros pueden hacer uso de bases de datos como [Azure SQL Database](https://docs.microsoft.com/azure/sql-database/?WT.mc_id=riskmodel-docs-scseely), [MySQL](https://docs.microsoft.com/azure/mysql/?WT.mc_id=riskmodel-docs-scseely) o [PostgreSQL](https://docs.microsoft.com/azure/postgresql/?WT.mc_id=riskmodel-docs-scseely). Puede usar una versión de bajo costo de cualquiera de estos elementos y luego escalar verticalmente el rendimiento durante una ejecución de modelado. Esto le proporciona el precio que necesita para el trabajo diario, además de la velocidad adicional justo cuando miles de núcleos solicitan datos. Normalmente, estos datos serán de solo lectura durante una ejecución de modelado. Si sus cálculos se realizan en varias regiones, considere la posibilidad de usar [Cosmos DB](https://docs.microsoft.com/azure/cosmos-db/distribute-data-globally?WT.mc_id=riskmodel-docs-scseely) o la [replicación geográfica de Azure SQL](https://docs.microsoft.com/azure/sql-database/sql-database-geo-replication-overview?WT.mc_id=riskmodel-docs-scseely). Ambos proporcionan mecanismos para replicar automáticamente datos en regiones con baja latencia. Su elección depende de las herramientas que los desarrolladores conozcan, de cómo ha modelado sus datos y del número de regiones utilizadas para la ejecución del modelado.

Dedique algún tiempo a pensar acerca de dónde se almacenarán los datos. Comprenda cuántas solicitudes simultáneas de los mismos datos existirán. Piense en cómo distribuirá la información:

- ¿Cada nodo computacional obtiene su propia copia?
- ¿Se comparte la copia a través de alguna ubicación con alto ancho de banda?

Si mantiene los datos centralizados mediante Azure SQL, es probable que mantenga la base de datos en un nivel de precio más bajo la mayor parte del tiempo. Si los datos solo se usan durante un ejecución de modelado y no se actualizan con frecuencia, los clientes de Azure harán incluso una copia de seguridad de los datos y desactivarán sus instancias de base de datos entre las ejecuciones. Los ahorros potenciales son grandes. Los clientes también pueden hacer uso de [grupos elásticos de Azure SQL](https://docs.microsoft.com/azure/sql-database/sql-database-elastic-pool?WT.mc_id=riskmodel-docs-scseely). Estos se diseñan para controlar los costos de las bases de datos, especialmente cuando no sabe qué bases de datos estarán bajo mucha carga en diferentes momentos. Los grupos elásticos permiten que una colección de bases de datos utilicen tanta potencia como necesiten y luego se reducen una vez que la demanda cambia en otras partes del sistema.

Es posible que deba deshabilitar la sincronización de datos durante una ejecución de modelado para que los cálculos posteriores del proceso utilicen los mismos datos. Si está utilizando la puesta en cola, deshabilite los procesadores de mensajes pero permita que las colas reciban datos.

También puede usar el tiempo antes de la ejecución para generar escenarios económicos, actualizar suposiciones actuariales y, en general, actualizar otros datos estáticos. Echemos un vistazo a la generación de escenarios económicos (ESG). La [Sociedad de Actuarios](https://www.soa.org/) proporciona el [Generador de Tasas de Interés de la Academia](https://www.soa.org/tables-calcs-tools/research-scenario/) (AIRG), un generador de escenarios económicos que modela los rendimientos del Tesoro de los Estados Unidos. AIRG se recomienda para uso en artículos como los cálculos del Manual de valoración 20 (VM-20). Otros generadores de escenarios económicos pueden modelar el mercado de valores, las hipotecas, los precios de los productos, etc.

Debido a que su entorno está procesando previamente los datos, también puede ejecutar otras partes antes. Por ejemplo, puede tener cosas que usted modele que usan registros para representar poblaciones más grandes. Uno normalmente hace esto agrupando en clústeres los registros. Si el conjunto de datos se actualiza esporádicamente, por ejemplo, una vez al día, se puede reducir el conjunto de registros a lo que se usará en el modelo como parte del proceso de ingesta.

Veamos un ejemplo práctico. Con NIIF-17, necesita agrupar sus contratos de manera que la distancia máxima entre las fechas de inicio de dos contratos cualesquiera sea inferior a un año. Supongamos que lo hace de forma sencilla y utiliza el año de contrato como mecanismo de agrupación. Esta segmentación se puede hacer mientras los datos se cargan en Azure leyendo el archivo y moviendo los registros a las agrupaciones de año apropiadas.

Centrarse en la preparación de datos reduce el tiempo necesario para ejecutar los componentes del modelo. Al obtener los datos pronto, puede ahorrar tiempo para ejecutar los modelos.

### <a name="parallelization"></a>Paralelización

La correcta paralelización de los pasos puede reducir drásticamente el tiempo de ejecución. Esta aceleración se produce optimizando las partes que implementa y sabiendo cómo expresar el modelo de una manera que permita que dos o más actividades se ejecuten simultáneamente. El truco es encontrar el equilibrio entre el tamaño de la solicitud de trabajo y la productividad de un nodo individual. Si la tarea pasa más tiempo en la configuración y la limpieza que en la evaluación, se ha quedado corto. Si la tarea es demasiado grande, el tiempo de ejecución no mejora. Desea que la actividad sea lo suficientemente pequeña como para repartirse por varios nodos y tener un efecto positivo en el tiempo de ejecución transcurrido.

Para aprovechar al máximo el sistema, debe comprender el flujo de trabajo del modelo y cómo interactúan los cálculos con la capacidad de escalar horizontalmente. Su software puede tener una noción de trabajos, tareas o algo similar. Use ese conocimiento para diseñar algo que pueda dividir el trabajo. Si tiene algunos pasos personalizados en el modelo, diséñelos para permitir que las entradas se dividan en grupos más pequeños para su procesamiento. A menudo, esto se conoce como un patrón de dispersión-recopilación.

- Dispersión: dividir las entradas a lo largo de líneas naturales y permitir la ejecución de tareas independientes.
- Recopilación: cuando las tareas se completan, recopilar sus salidas.

Al dividir las cosas, sepa también dónde debe sincronizarse el proceso antes de seguir adelante. Hay algunos lugares comunes donde la gente divide las cosas. Para ejecuciones estocásticas anidadas, puede tener mil bucles externos con un conjunto de puntos de inflexión que ejecutan bucles internos de cien escenarios. Cada bucle externo puede ejecutarse simultáneamente. Detiene en un punto de inflexión y luego ejecuta los bucles internos simultáneamente, recupera la información para ajustar los datos para el bucle externo y sigue avanzando. En la siguiente figura se muestra el flujo de trabajo. Dado el cálculo suficiente, puede ejecutar los 100 000 bucles internos en 100 000 núcleos, lo que reduce el tiempo de procesamiento a la suma de los siguientes tiempos:

![](./assets/insurance-risk-assets/timing.png)

La distribución aumentará un poco dependiendo de cómo se haga; puede ser tan simple como construir un trabajo pequeño con los parámetros correctos o tan compleja como copiar 100 000 archivos en los lugares adecuados. El procesamiento de los resultados se puede incluso acelerar si puede distribuir la agregación de resultados mediante Apache Spark de HD Insight, Azure Databricks o su propia implementación. Por ejemplo, calcular los promedios es simplemente una cuestión de recordar el número de elementos vistos hasta ahora y la suma. Otros cálculos pueden funcionar mejor en una sola máquina con miles de núcleos. Para aquellos, puede hacer uso de máquinas habilitadas para GPU en Azure.

La mayoría de los equipos actuariales inician este recorrido moviendo sus modelos a Azure. Después recopilan datos de tiempo en los distintos pasos del proceso. Más adelante, ordenan la hora para cada paso desde el tiempo transcurrido mayor al menor. No se fijarán en el tiempo total de ejecución, ya que algo puede consumir miles de horas de núcleo pero solo un tiempo transcurrido de 20 minutos. Para cada uno de los pasos de trabajo de ejecución más largos, los desarrolladores actuariales buscan formas de disminuir el tiempo transcurrido y, simultáneamente, obtener los resultados correctos. Este proceso se repite regularmente. Algunos equipos actuariales establecerán un tiempo de ejecución a modo de objetivo; por ejemplo, un análisis de cobertura durante toda la noche tiene el objetivo de ejecutarse en menos de 8 horas. Tan pronto como el tiempo supere las 8,25 horas, una parte del equipo actuarial cambiará para mejorar el tiempo de la parte más larga en el análisis. Cuando el tiempo pase a ser de menos de 7,5 horas, volverán al desarrollo. La heurística para volver atrás y optimizar varía entre los actuarios.

Para ejecutar todo esto, tiene varias opciones. La mayoría del software actuarial trabaja con las cuadrículas de proceso. Las cuadrículas que funcionan en local y en Azure, usan [HPC Pack](https://docs.microsoft.com/azure/virtual-machines/windows/hpcpack-cluster-options?WT.mc_id=riskmodel-docs-scseely), un paquete de terceros o algo personalizado. Las cuadrículas optimizadas para Azure usarán [Virtual Machine Scale Sets](https://docs.microsoft.com/azure/virtual-machine-scale-sets/?WT.mc_id=riskmodel-docs-scseely), [Batch](https://docs.microsoft.com/azure/batch/?WT.mc_id=riskmodel-docs-scseely) o algo personalizado. Si decide usar Scale Sets o Batch, asegúrese de fijarse en su compatibilidad con máquinas virtuales de baja prioridad (documentos de baja prioridad de [Scale Sets](https://docs.microsoft.com/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-use-low-priority?WT.mc_id=riskmodel-docs-scseely) y documentos de baja prioridad de [Batch](https://docs.microsoft.com/azure/batch/batch-low-pri-vms?WT.mc_id=riskmodel-docs-scseely)). Una máquina virtual de baja prioridad es una máquina virtual que se ejecuta en hardware que puede alquilar por una fracción del precio normal. El precio más bajo está disponible porque las máquinas virtuales de baja prioridad pueden evitarse cuando la capacidad lo exige. Si tiene algún margen de maniobra en su presupuesto de tiempo, las máquinas virtuales de baja prioridad proporcionan una manera excelente de reducir el precio de una ejecución de modelado.

Si necesita coordinar la ejecución y la implementación en muchas máquinas, tal vez con algunas ejecutándose en diferentes regiones, puede aprovechar CycleCloud. CycleCloud no tiene ningún costo adicional. Permite organizar el movimiento de datos cuando es necesario. Esto incluye la asignación, supervisión y apagado de las máquinas. Incluso puede controlar las máquinas de baja prioridad, asegurándose de la contención de los gastos. Puede ir tan lejos como para describir la combinación de máquinas que necesita. Por ejemplo, quizás necesita una clase de máquina pero la ejecución se puede realizar correctamente en cualquier versión que tenga 2 núcleos o más. CycleCloud puede asignar los núcleos a través de esos tipos de máquina.

## <a name="reporting-on-the-results"></a>Creación de informes sobre los resultados

Una vez que los paquetes actuariales se han ejecutado y han generado los resultados correspondientes, tendrá varios informes preparados para el regulador. También tendrá una enorme cantidad de datos nuevos que tal vez desee analizar para generar información que los reguladores o auditores no requieren. Es posible que desee comprender el perfil de sus mejores clientes. Con las conclusiones, puede indicarle a marketing cómo es un cliente de bajo costo para que los encargados del marketing y las ventas puedan encontrarlos más rápido. Del mismo modo, puede usar los datos para descubrir qué grupos se benefician más de tener el seguro. Por ejemplo, puede descubrir que las personas que aprovechan un examen físico anual se interesaron por los problemas de salud en una etapa temprana. Esto ahorra tiempo y dinero a las compañías de seguros. Puede usar esos datos para controlar el comportamiento de la base de clientes.

Para ello, le interesará acceder a una gran cantidad de herramientas de ciencia de datos, así como a algunas partes para la visualización. Dependiendo de la cantidad de investigación que desee realizar, puede comenzar con [Data Science VM](https://docs.microsoft.com/azure/machine-learning/data-science-virtual-machine/overview?WT.mc_id=riskmodel-docs-scseely), que se puede aprovisionar desde Azure Marketplace. Estas máquinas virtuales tienen versiones de Windows y Linux. Verá que se han instalado Microsoft R Open, Microsoft ML Server, Anaconda, Jupyter y otras herramientas listas para usarse. Agregue R o Python para visualizar los datos y compartir información con sus colegas.

Si necesita hacer más análisis, puede usar las herramientas de ciencia de datos de Apache, como Spark, Hadoop y otras, a través de HDInsight o Databricks. Utilícelas más cuando el análisis deba realizarse regularmente y desee automatizar el flujo de trabajo. Esto también es útil para el análisis de grandes conjuntos de datos en directo.

Una vez haya encontrado algo interesante, deberá presentar los resultados. Muchos actuarios empezarán tomando los resultados de muestra y llevándolos a Excel para crear gráficos y otras visualizaciones. Si desea algo que también tenga una bonita interfaz para la obtención de detalles de los datos, eche un vistazo a [Power BI](https://docs.microsoft.com/power-bi/?WT.mc_id=riskmodel-docs-scseely). Power BI puede hacer que algunas visualizaciones resulten agradables, mostrar los datos de origen y permite explicar los datos al lector mediante la incorporación de [marcadores ordenados y anotados](https://docs.microsoft.com/power-bi/desktop-bookmarks?WT.mc_id=riskmodel-docs-scseely).

## <a name="data-retention"></a>Retención de datos

Muchos de los datos que se colocan en el sistema deben conservarse para futuras auditorías. Los requisitos de la retención de datos normalmente oscilan entre 7 y 10 años, pero varían. La retención mínima implica:

- Una instantánea de las entradas originales en el modelo. Esto incluye activos, pasivos, suposiciones, generadores de escenarios económicos y otras entradas.
- Instantánea de las salidas finales. Esto incluye los datos utilizados para crear informes que se presentan a los organismos reguladores.
- Otros resultados intermedios importantes. Un auditor le preguntará por qué su modelo generó algún resultado. Debe conservar la evidencia sobre por qué el modelo tomó ciertas decisiones o presentó números concretos. Muchas aseguradoras optarán por mantener los binarios usados para generar las salidas finales a partir de las entradas originales. Luego, cuando se les pregunte, volverán a ejecutar el modelo para obtener una copia actualizada de los resultados intermedios. Si las salidas coinciden, entonces los archivos intermedios también deben contener las explicaciones que necesitan.

Durante la ejecución del modelo, los actuarios usan mecanismos de entrega de datos que pueden controlar la carga de solicitudes desde la ejecución. Una vez completada la ejecución y que los datos ya no son necesarios, conservan algunos de los datos. Como mínimo, una aseguradora debe conservar las entradas y la configuración del entorno de ejecución para cualquier requisito de reproducibilidad. Las bases de datos se conservan para hacer las copias de seguridad en Azure Blob Storage y los servidores se apagan. Los datos sobre el almacenamiento de alta velocidad también se trasladan a Azure Blob Storage, que es menos costoso. Una vez en Blob Storage, puede elegir el nivel de datos utilizado para cada blob: nivel de acceso frecuente, nivel de acceso esporádico o nivel de acceso de archivo. El almacenamiento de nivel de acceso frecuente funciona bien para archivos a los que se accede frecuente. El almacenamiento de nivel de acceso esporádico está optimizado para el acceso a datos poco frecuente. El almacenamiento de archivos es el mejor para mantener archivos auditables pero el ahorro de precio tiene un costo de latencia: la latencia de datos del nivel de almacenamiento de archivos se mide en horas. Lea [Azure Blob Storage: niveles de almacenamiento de archivo, esporádico, frecuente y premium (versión preliminar)](https://docs.microsoft.com/azure/storage/blobs/storage-blob-storage-tiers?WT.mc_id=riskmodel-docs-scseely) para comprender completamente los diferentes niveles de almacenamiento. Puede administrar los datos desde la creación hasta la eliminación con la administración del ciclo de vida. Los URI para los blobs permanecen estáticos, pero donde el blob se almacena se vuelve más barato con el paso del tiempo. Esta función ahorrará mucho dinero y quebraderos de cabeza para muchos usuarios de Azure Storage. Puede obtener información sobre los pormenores en [Administración del ciclo de vida de Azure Blob Storage (versión preliminar)](https://docs.microsoft.com/azure/storage/common/storage-lifecycle-managment-concepts?WT.mc_id=riskmodel-docs-scseely). El hecho de que pueda eliminar archivos automáticamente es maravilloso: significa que no expandirá una auditoría accidentalmente al referirse a un archivo que está fuera del ámbito porque el propio archivo puede eliminarse automáticamente.

## <a name="next-steps"></a>Pasos siguientes

Si el sistema actuarial que ejecuta tiene una implementación de cuadrícula local, es probable que esa implementación también se ejecute en Azure. Para algunos proveedores, tienen implementaciones de Azure especializadas que se ejecutan en hiperescala. Como parte del traslado a Azure, mueva también sus herramientas internas. Los actuarios de cualquier parte han descubierto que sus aptitudes de ciencia de datos funcionan bien en sus equipos portátiles o en un entorno amplio. Busque las cosas que su equipo ya hace: tal vez tenga algo que usa un aprendizaje profundo pero que tarda horas o días en ejecutarse en una GPU. Intente ejecutar la misma carga de trabajo en una máquina con 4 GPU de gama alta y observe los tiempos de ejecución; lo más probable es que vea aceleraciones significativas para cosas que ya tiene.

A medida que las cosas mejoren, asegúrese de que también crea algo de sincronización de datos para alimentar los datos de modelado. Una ejecución del modelo no puede comenzar hasta que los datos estén listos. Esto puede implicar dedicar un poco más de esfuerzo para que solo envíe datos que hayan cambiado. El enfoque real también depende del tamaño de los datos: actualizar unos pocos MB quizás no sea un gran problema, pero reducir la cantidad de cargas de gigabytes acelerará mucho las cosas.

### <a name="tutorials"></a>Tutoriales

- Desarrolladores de R: [Ejecución de una simulación de R en paralelo con Azure Batch](https://docs.microsoft.com/azure/batch/tutorial-r-doazureparallel?WT.mc_id=riskmodel-docs-scseely)
- Tutorial para mostrar cómo usar una instancia de Azure Functions para interactuar con el almacenamiento: [Carga de imágenes en Blob Storage con Azure Functions](https://docs.microsoft.com/azure/functions/tutorial-static-website-serverless-api-with-database?tutorial-step=2&WT.mc_id=riskmodel-docs-scseely)
- ETL con Databricks: [Extracción, transformación y carga de datos mediante Azure Databricks](https://docs.microsoft.com/azure/azure-databricks/databricks-extract-load-sql-data-warehouse?WT.mc_id=riskmodel-docs-scseely)
- ETL con HDInsight: [Realización de las operaciones de extracción, transformación y carga mediante Apache Hive en Azure HDInsight](https://docs.microsoft.com/azure/hdinsight/hdinsight-analyze-flight-delay-data-linux?toc=%2Fen-us%2Fazure%2Fhdinsight%2Fhadoop%2FTOC.json&amp;bc=%2Fen-us%2Fazure%2Fbread%2Ftoc.json&WT.mc_id=riskmodel-docs-scseely)
- Ciencia de datos con una instancia de Data Science Virtual Machine de Linux en Azure: [https://docs.microsoft.com/azure/machine-learning/data-science-virtual-machine/linux-dsvm-walkthrough](https://docs.microsoft.com/azure/machine-learning/data-science-virtual-machine/linux-dsvm-walkthrough?WT.mc_id=riskmodel-docs-scseely)
- Diez cosas que puede hacer en Windows Data Science Virtual Machine: [https://docs.microsoft.com/azure/machine-learning/data-science-virtual-machine/vm-do-ten-things](https://docs.microsoft.com/azure/machine-learning/data-science-virtual-machine/vm-do-ten-things?WT.mc_id=riskmodel-docs-scseely)
