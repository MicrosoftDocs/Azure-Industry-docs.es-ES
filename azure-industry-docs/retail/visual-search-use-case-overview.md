---
title: Información básica sobre Visual Search en el comercio minorista con CosmosDB
author: scseely
ms.author: scseely
ms.date: 11/20/2019
ms.topic: article
ms.service: industry
description: En este artículo se explican las fases de migración de la infraestructura de comercio electrónico del entorno local a Azure.
ms.openlocfilehash: b43ea305e11ac32da58e4d0521d79f90d5c23d85
ms.sourcegitcommit: 2714a77488c413f01beb169a18acab45663bcfd7
ms.translationtype: HT
ms.contentlocale: es-ES
ms.lasthandoff: 11/21/2019
ms.locfileid: "74308482"
---
# <a name="visual-search-overview"></a><span data-ttu-id="63d13-103">Introducción a Visual Search</span><span class="sxs-lookup"><span data-stu-id="63d13-103">Visual Search Overview</span></span>

## <a name="executive-summary"></a><span data-ttu-id="63d13-104">Resumen ejecutivo</span><span class="sxs-lookup"><span data-stu-id="63d13-104">Executive Summary</span></span>

<span data-ttu-id="63d13-105">Como todos ya sabemos, la inteligencia artificial ofrece el potencial de transformar el comercio minorista.</span><span class="sxs-lookup"><span data-stu-id="63d13-105">Artificial Intelligence offers the potential to transform retailing as we know it today.</span></span> <span data-ttu-id="63d13-106">Es razonable creer que los minoristas desarrollarán una arquitectura de la experiencia del cliente respaldada por la inteligencia artificial.</span><span class="sxs-lookup"><span data-stu-id="63d13-106">It is reasonable to believe that retailers will develop a customer experience architecture supported by AI.</span></span> <span data-ttu-id="63d13-107">Algunas de las expectativas son que una plataforma mejorada con inteligencia artificial proporcionará un aumento notable de los ingresos debido a la hiperpersonalización.</span><span class="sxs-lookup"><span data-stu-id="63d13-107">Some expectations are that a platform enhanced with AI will provide a revenue bump due to hyper personalization.</span></span> <span data-ttu-id="63d13-108">El comercio digital continúa reforzando las expectativas, las preferencias y el comportamiento del cliente.</span><span class="sxs-lookup"><span data-stu-id="63d13-108">Digital commerce continues to heighten customer expectations, preferences and behavior.</span></span> <span data-ttu-id="63d13-109">Demandas como interacción en tiempo real, recomendaciones pertinentes e hiperpersonalización están impulsando la velocidad y la comodidad al clic de un botón.</span><span class="sxs-lookup"><span data-stu-id="63d13-109">Demands such as real-time engagement, relevant recommendations and hyper-personalization are driving speed and convenience at a click of a button.</span></span> <span data-ttu-id="63d13-110">La habilitación de la inteligencia en las aplicaciones mediante voz natural, visión, etc. permite mejoras en el sector minorista que aumentarán el valor y afectaran al modo en que los clientes compran.</span><span class="sxs-lookup"><span data-stu-id="63d13-110">Enabling intelligence in applications through natural speech, vision, etc. enables improvements in retail that will increase value while disrupting how customers shop.</span></span>

<span data-ttu-id="63d13-111">Este documento se centra en el concepto de inteligencia artificial de **búsqueda visual** y ofrece algunas consideraciones importantes sobre su implementación.</span><span class="sxs-lookup"><span data-stu-id="63d13-111">This document focuses on the AI  concept of **visual search** and offers a few key considerations on its implementation.</span></span> <span data-ttu-id="63d13-112">Se proporciona un flujo de trabajo de ejemplo y se asignan sus fases a las tecnologías de Azure pertinentes.</span><span class="sxs-lookup"><span data-stu-id="63d13-112">It provides a workflow example and maps its stages to the relevant Azure technologies.</span></span> <span data-ttu-id="63d13-113">El concepto se basa en un cliente que puede aprovechar una imagen tomada con su dispositivo móvil o ubicada en Internet para realizar una búsqueda de artículos pertinentes o parecidos según la intención de la experiencia.</span><span class="sxs-lookup"><span data-stu-id="63d13-113">The concept is based on a customer being able to leverage an image taken with their mobile device or located on the internet to conduct a search of relevant and/or like items, depending upon the intention of the experience.</span></span> <span data-ttu-id="63d13-114">Por lo tanto, la búsqueda visual mejora la velocidad desde la entrada de texto a una imagen con varios puntos de metadatos a fin de que aparezcan rápidamente los artículos aplicables disponibles.</span><span class="sxs-lookup"><span data-stu-id="63d13-114">Thus, visual search improves speed from texted entry to an image with multiple meta-data points to quickly surface applicable items available.</span></span>

## <a name="visual-search-engines"></a><span data-ttu-id="63d13-115">Motores de búsqueda visual</span><span class="sxs-lookup"><span data-stu-id="63d13-115">Visual Search Engines</span></span>

<span data-ttu-id="63d13-116">Los motores de búsqueda visual recuperan información mediante el uso de imágenes como entrada y, con frecuencia, aunque no exclusivamente, también como salida.</span><span class="sxs-lookup"><span data-stu-id="63d13-116">Visual search engines retrieve information using images as input and often—but not exclusively—as output too.</span></span>

<span data-ttu-id="63d13-117">Los motores son cada vez más comunes en el sector minorista y por muy buenas razones:</span><span class="sxs-lookup"><span data-stu-id="63d13-117">Engines are becoming more and more common in the retail industry, and for very good reasons:</span></span>

- <span data-ttu-id="63d13-118">Según un informe publicado por [Emarketer](https://www.emarketer.com/Report/Visual-Commerce-2017-How-Image-Recognition-Augmentation-Changing-Retail/2002059) en 2017, casi un 75 % de los usuarios de Internet buscan imágenes o vídeos de un producto antes de realizar una compra.</span><span class="sxs-lookup"><span data-stu-id="63d13-118">Around 75% of internet users search for pictures or videos of a product before making a purchase, according to an [Emarketer](https://www.emarketer.com/Report/Visual-Commerce-2017-How-Image-Recognition-Augmentation-Changing-Retail/2002059) report published in 2017.</span></span>
- <span data-ttu-id="63d13-119">De acuerdo con el informe de [Slyce](https://slyce.it/wp-content/uploads/2015/11/Visual_Search_Technology_and_Market.pdf) (una compañía de búsqueda visual) de 2015, el 74 % de los consumidores también considera que las búsquedas de texto no son eficaces.</span><span class="sxs-lookup"><span data-stu-id="63d13-119">74% of consumers also find text searches inefficient, according to [Slyce](https://slyce.it/wp-content/uploads/2015/11/Visual_Search_Technology_and_Market.pdf) (a visual search company) 2015 report.</span></span>

<span data-ttu-id="63d13-120">Por lo tanto, según la investigación realizada por [Markets &amp; Markets](https://www.marketsandmarkets.com/PressReleases/image-recognition.asp), el mercado de reconocimiento de imágenes representará más de 25 000 millones de dólares USD para el año 2019.</span><span class="sxs-lookup"><span data-stu-id="63d13-120">Therefore, the image recognition market will be worth more than $25 billion by 2019, according to research by [Markets &amp; Markets](https://www.marketsandmarkets.com/PressReleases/image-recognition.asp).</span></span>

<span data-ttu-id="63d13-121">La tecnología ya ha arraigado en las principales marcas de comercio electrónico, quienes también han contribuido significativamente a su desarrollo.</span><span class="sxs-lookup"><span data-stu-id="63d13-121">The technology has already taken hold with major e-commerce brands, who have also contributed significantly to its development.</span></span> <span data-ttu-id="63d13-122">Los usuarios pioneros más importantes son probablemente:</span><span class="sxs-lookup"><span data-stu-id="63d13-122">The most prominent early adopters are probably:</span></span>

- <span data-ttu-id="63d13-123">eBay con sus herramientas Image Search y "Find It on eBay" de la aplicación (por ahora solo una experiencia móvil).</span><span class="sxs-lookup"><span data-stu-id="63d13-123">eBay with their Image Search and "Find It on eBay" tools in their app (this is currently only a mobile experience).</span></span>
- <span data-ttu-id="63d13-124">Pinterest con su herramienta de detección visual, Lens.</span><span class="sxs-lookup"><span data-stu-id="63d13-124">Pinterest with their Lens visual discovery tool.</span></span>
- <span data-ttu-id="63d13-125">Microsoft con Bing Visual Search.</span><span class="sxs-lookup"><span data-stu-id="63d13-125">Microsoft with Bing Visual Search.</span></span>

## <a name="adopt-and-adapt"></a><span data-ttu-id="63d13-126">Adopción y adaptación</span><span class="sxs-lookup"><span data-stu-id="63d13-126">Adopt and Adapt</span></span>

<span data-ttu-id="63d13-127">Afortunadamente, no se necesitan grandes cantidades de potencia de computación para beneficiarse de la búsqueda visual.</span><span class="sxs-lookup"><span data-stu-id="63d13-127">Fortunately, you don't need vast amounts of computing power to profit from visual search.</span></span> <span data-ttu-id="63d13-128">Cualquier negocio con un catálogo de imágenes puede sacar partido de la experiencia de inteligencia artificial de Microsoft integrada en sus servicios de Azure.</span><span class="sxs-lookup"><span data-stu-id="63d13-128">Any business with an image catalog can take advantage of Microsoft's AI expertise built into its Azure services.</span></span>

<span data-ttu-id="63d13-129">[Bing Visual Search](https://azure.microsoft.com/services/cognitive-services/bing-visual-search/?WT.mc_id=vsearchgio-article-gmarchet) API proporciona una manera de extraer información de contexto de las imágenes e identificar, por ejemplo, mobiliario para el hogar, moda, varias clases de productos, etc.</span><span class="sxs-lookup"><span data-stu-id="63d13-129">[Bing Visual Search](https://azure.microsoft.com/services/cognitive-services/bing-visual-search/?WT.mc_id=vsearchgio-article-gmarchet) API provides a way to extract context information from images, identifying—for instance—home furnishings, fashion, several kinds of products, etc.</span></span>

<span data-ttu-id="63d13-130">También devuelve imágenes de su propio catálogo que son visualmente parecidas, productos con orígenes de compra relativos o búsquedas relacionadas.</span><span class="sxs-lookup"><span data-stu-id="63d13-130">It will also return visually similar images out of its own catalog, products with relative shopping sources, related searches.</span></span> <span data-ttu-id="63d13-131">Si bien es interesante, su uso será limitado si la empresa no es uno de esos orígenes.</span><span class="sxs-lookup"><span data-stu-id="63d13-131">While interesting, this will be of limited use if your company is not one of those sources.</span></span>

<span data-ttu-id="63d13-132">Bing también proporciona:</span><span class="sxs-lookup"><span data-stu-id="63d13-132">Bing will also provide:</span></span>

- <span data-ttu-id="63d13-133">Etiquetas que le permiten explorar objetos o conceptos que se encuentran en la imagen.</span><span class="sxs-lookup"><span data-stu-id="63d13-133">Tags that allow you to explore objects or concepts found in the image.</span></span>
- <span data-ttu-id="63d13-134">Rectángulos de selección para regiones de interés de la imagen (por ejemplo, artículos de ropa o muebles).</span><span class="sxs-lookup"><span data-stu-id="63d13-134">Bounding boxes for regions of interest in the image (e.g. clothing, furniture items).</span></span>

<span data-ttu-id="63d13-135">Puede tomar esa información para reducir considerablemente el espacio (y el tiempo) de búsqueda en el catálogo de productos de la empresa y limitarlo a los objetos que son como los de la región y categoría de interés.</span><span class="sxs-lookup"><span data-stu-id="63d13-135">You can take that information to reduce the search space (and time) into a company's product catalog significantly, restricting it to objects like those in the region and category of interest.</span></span>

## <a name="implement-your-own"></a><span data-ttu-id="63d13-136">Implementación por su cuenta</span><span class="sxs-lookup"><span data-stu-id="63d13-136">Implement Your Own</span></span>

<span data-ttu-id="63d13-137">Hay algunos componentes clave que debe tener en cuenta al implementar la búsqueda visual:</span><span class="sxs-lookup"><span data-stu-id="63d13-137">There are a few key components to consider when implementing visual search:</span></span>

- <span data-ttu-id="63d13-138">Ingesta y filtrado de imágenes</span><span class="sxs-lookup"><span data-stu-id="63d13-138">Ingesting and filtering images</span></span>
- <span data-ttu-id="63d13-139">Técnicas de almacenamiento y recuperación</span><span class="sxs-lookup"><span data-stu-id="63d13-139">Storage and retrieval techniques</span></span>
- <span data-ttu-id="63d13-140">Caracterización, codificación o "configuración de hash"</span><span class="sxs-lookup"><span data-stu-id="63d13-140">Featurization, encoding or "hashing"</span></span>
- <span data-ttu-id="63d13-141">Medidas o distancias de similitud y clasificación</span><span class="sxs-lookup"><span data-stu-id="63d13-141">Similarity measures or distances and ranking</span></span>

 ![](./assets/visual-search-use-case-overview/visual-search-pipeline.png)

<span data-ttu-id="63d13-142">*Figura 1: Ejemplo de canalización de Visual Search*</span><span class="sxs-lookup"><span data-stu-id="63d13-142">*Figure 1: Example of Visual Search Pipeline*</span></span>

### <a name="sourcing-the-pictures"></a><span data-ttu-id="63d13-143">Origen de las imágenes</span><span class="sxs-lookup"><span data-stu-id="63d13-143">Sourcing the Pictures</span></span>

<span data-ttu-id="63d13-144">Si no dispone de un catálogo de imágenes, es posible que deba entrenar los algoritmos en conjuntos de datos disponibles públicamente, como fashion [MNIST](https://www.kaggle.com/zalando-research/fashionmnist), deep [fashion](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) y similar.</span><span class="sxs-lookup"><span data-stu-id="63d13-144">If you do not own a picture catalog, you may need to train the algorithms on openly available data sets, such as fashion [MNIST](https://www.kaggle.com/zalando-research/fashionmnist), deep [fashion](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) and similar.</span></span> <span data-ttu-id="63d13-145">Estos algoritmos contienen varias categorías de productos y se usan normalmente para comparar los algoritmos de clasificación y búsqueda de imágenes.</span><span class="sxs-lookup"><span data-stu-id="63d13-145">They contain several categories of products and are commonly used to benchmark image categorization and search algorithms.</span></span>

 ![](./assets/visual-search-use-case-overview/deep-fashion-dataset.png)

<span data-ttu-id="63d13-146">*Figura 2: Ejemplo del conjunto de datos Deep Fashion*</span><span class="sxs-lookup"><span data-stu-id="63d13-146">*Figure 2: An Example from The Deep Fashion Dataset*</span></span>

### <a name="filtering-the-images"></a><span data-ttu-id="63d13-147">Filtrado de las imágenes</span><span class="sxs-lookup"><span data-stu-id="63d13-147">Filtering the Images</span></span>

<span data-ttu-id="63d13-148">La mayoría de los conjuntos de datos de referencia, como los antes mencionados, se han procesado previamente.</span><span class="sxs-lookup"><span data-stu-id="63d13-148">Most benchmark datasets - such as those mentioned before - have already been pre-processed.</span></span>

<span data-ttu-id="63d13-149">Si va a compilar el suyo propio, como mínimo, le interesará que todas las imágenes tengan el mismo tamaño, determinado principalmente por la entrada para la que se entrena el modelo.</span><span class="sxs-lookup"><span data-stu-id="63d13-149">If you are building your own, at a minimum you will want the images to all have the same size, mostly dictated by the input that your model is trained for.</span></span>

<span data-ttu-id="63d13-150">En muchos casos, es mejor también normalizar la luminosidad de las imágenes.</span><span class="sxs-lookup"><span data-stu-id="63d13-150">In many cases, it is best also to normalize the luminosity of the images.</span></span> <span data-ttu-id="63d13-151">Según el nivel de detalle de la búsqueda, color puede ser también información redundante, por lo que reducirlo a blanco y negro le ayudará con los tiempos de procesamiento.</span><span class="sxs-lookup"><span data-stu-id="63d13-151">Depending on the detail level of your search, color may also be redundant information, so reducing to black and white will help with processing times.</span></span>

<span data-ttu-id="63d13-152">Por último, pero no menos importante, el conjunto de datos de imagen debe estar equilibrado entre las diferentes clases que representa.</span><span class="sxs-lookup"><span data-stu-id="63d13-152">Last but not least, the image dataset should be balanced across the different classes it represents.</span></span>

### <a name="image-database"></a><span data-ttu-id="63d13-153">Base de datos de imágenes</span><span class="sxs-lookup"><span data-stu-id="63d13-153">Image Database</span></span>

<span data-ttu-id="63d13-154">La capa de datos es un componente especialmente delicado de su arquitectura.</span><span class="sxs-lookup"><span data-stu-id="63d13-154">The data layer is a particularly delicate component of your architecture.</span></span> <span data-ttu-id="63d13-155">Contendrá:</span><span class="sxs-lookup"><span data-stu-id="63d13-155">It will contain:</span></span>

- <span data-ttu-id="63d13-156">Imágenes</span><span class="sxs-lookup"><span data-stu-id="63d13-156">Images</span></span>
- <span data-ttu-id="63d13-157">Los metadatos sobre las imágenes (tamaño, etiquetas, SKU del producto o descripción)</span><span class="sxs-lookup"><span data-stu-id="63d13-157">Any metadata about the images (size, tags, product SKUs, description)</span></span>
- <span data-ttu-id="63d13-158">Los datos generados por el modelo de aprendizaje automático (por ejemplo, un vector numérico de 4096 elementos por imagen)</span><span class="sxs-lookup"><span data-stu-id="63d13-158">Data generated by the machine learning model (for instance a 4096-element numerical vector  per image)</span></span>

<span data-ttu-id="63d13-159">A medida que recupera imágenes de distintos orígenes o usa modelos de aprendizaje automático para obtener un rendimiento óptimo, la estructura de los datos cambia.</span><span class="sxs-lookup"><span data-stu-id="63d13-159">As you retrieve images from different sources or use several machine learning models for optimal performance, the structure of the data will change.</span></span> <span data-ttu-id="63d13-160">Por lo tanto, es importante elegir una tecnología o una combinación que pueda tratar con datos semiestructurados y sin esquema fijo.</span><span class="sxs-lookup"><span data-stu-id="63d13-160">It is therefore important to choose a technology or combination that can deal with semi-structured data and no fixed schema.</span></span>

<span data-ttu-id="63d13-161">También puede necesitar un número mínimo de puntos de datos útiles (por ejemplo, un identificador o clave de imagen, una SKU de producto, una descripción o un campo de etiqueta).</span><span class="sxs-lookup"><span data-stu-id="63d13-161">You may also want to require a minimum number of useful data points (e.g. an image identifier or key, a product sku, a description, a tag field).</span></span>

<span data-ttu-id="63d13-162">[Azure Cosmos DB](https://azure.microsoft.com/services/cosmos-db/?WT.mc_id=vsearchgio-article-gmarchet) ofrece la flexibilidad necesaria, junto con diversos mecanismos de acceso para las aplicaciones basadas en este servicio (lo que ayudará a la búsqueda en el catálogo).</span><span class="sxs-lookup"><span data-stu-id="63d13-162">[Azure CosmosDB](https://azure.microsoft.com/services/cosmos-db/?WT.mc_id=vsearchgio-article-gmarchet) offers the required flexibility and a variety of access mechanisms for applications built on top of it (which will help with your catalog search).</span></span> <span data-ttu-id="63d13-163">Sin embargo, hay que tener cuidado para alcanzar la mejor relación precio/rendimiento.</span><span class="sxs-lookup"><span data-stu-id="63d13-163">However, one has to be careful to drive the best price/performance.</span></span> <span data-ttu-id="63d13-164">Cosmos DB permite el almacenamiento de los datos adjuntos de los documentos, pero hay un límite total por cuenta y puede ser una propuesta costosa.</span><span class="sxs-lookup"><span data-stu-id="63d13-164">CosmosDB allows document attachments to be stored, but there is a total limit per account and it may be a costly proposition.</span></span> <span data-ttu-id="63d13-165">Una práctica común es almacenar los archivos de imagen reales en blobs e insertar un vínculo a ellos en la base de datos.</span><span class="sxs-lookup"><span data-stu-id="63d13-165">It is common practice to store the actual image files in blobs and insert a link to them in the database.</span></span> <span data-ttu-id="63d13-166">En el caso de Cosmos DB, esto supone crear un documento que contenga las propiedades del catálogo asociadas a esa imagen (SKU, etiqueta, etc.) y un archivo adjunto que contenga la dirección URL del archivo de imagen (por ejemplo, en Azure Blob Storage, OneDrive, etc.).</span><span class="sxs-lookup"><span data-stu-id="63d13-166">In the case of CosmosDB this implies creating a document that contains the catalog properties associated to that image (sku, tag etc.) and an attachment that contains the URL of the image file (e.g. on Azure blob storage, OneDrive etc).</span></span>

 ![](./assets/visual-search-use-case-overview/cosmosdb-data-model.png)

<span data-ttu-id="63d13-167">*Figura 3: Modelo jerárquico de recursos de CosmosDB*</span><span class="sxs-lookup"><span data-stu-id="63d13-167">*Figure 3: CosmosDB Hierarchical Resource Model*</span></span>

<span data-ttu-id="63d13-168">Si tiene previsto aprovechar las ventajas de la distribución global de Cosmos DB, tenga en cuenta que se replicarán los documentos y datos adjuntos, pero no los archivos vinculados.</span><span class="sxs-lookup"><span data-stu-id="63d13-168">If you plan to take advantage of the global distribution of Cosmos DB, note that it will replicate the documents and attachments, but not the linked files.</span></span> <span data-ttu-id="63d13-169">Para ello, puede considerar una red de distribución de contenido.</span><span class="sxs-lookup"><span data-stu-id="63d13-169">You may want to consider a content distribution network for those.</span></span>

<span data-ttu-id="63d13-170">Otras tecnologías aplicables son una combinación de Azure SQL Database (si es aceptable el esquema fijo) y blobs, o incluso tablas de Azure y blobs para almacenamiento y recuperación de forma rápida y económica.</span><span class="sxs-lookup"><span data-stu-id="63d13-170">Other applicable technologies are a combination of Azure SQL Database (if fixed schema is acceptable) and blobs, or even Azure Tables and blobs for inexpensive and fast storage and retrieval.</span></span>

### <a name="feature-extraction-amp-encoding"></a><span data-ttu-id="63d13-171">Extracción de características y codificación</span><span class="sxs-lookup"><span data-stu-id="63d13-171">Feature Extraction &amp; Encoding</span></span>

<span data-ttu-id="63d13-172">El proceso de codificación extrae características destacadas de las imágenes de la base de datos y asigna a cada una de ellas un "vector de características" disperso (un vector con muchos ceros) que puede tener miles de componentes.</span><span class="sxs-lookup"><span data-stu-id="63d13-172">The encoding process extracts salient features from pictures in the database and maps each of them to a sparse "feature" vector (a vector with many zeros) that can have thousands of components.</span></span> <span data-ttu-id="63d13-173">Este vector es una representación numérica de las características (por ejemplo, bordes o formas) que caracterizan la imagen, de forma parecida a un código.</span><span class="sxs-lookup"><span data-stu-id="63d13-173">This vector is a numerical representation of the features (e.g. edges, shapes) that characterize the picture – akin to a code.</span></span>

<span data-ttu-id="63d13-174">Las técnicas de extracción de características usan normalmente _mecanismos de aprendizaje por transferencia_.</span><span class="sxs-lookup"><span data-stu-id="63d13-174">Feature extraction techniques typically use _transfer learning mechanisms_.</span></span> <span data-ttu-id="63d13-175">Esto tiene lugar cuando selecciona una red neuronal previamente entrenada, ejecuta cada imagen a través de ella y almacena el vector de características de nuevo en la base de datos de imágenes.</span><span class="sxs-lookup"><span data-stu-id="63d13-175">This occurs when you select a pre-trained neural network, run each image through it and store the feature vector  produced back in your image database.</span></span> <span data-ttu-id="63d13-176">En este modo, el aprendizaje se "transfiere" desde la persona que haya entrenado la red.</span><span class="sxs-lookup"><span data-stu-id="63d13-176">In that way, you "transfer" the learning from whoever trained the network.</span></span> <span data-ttu-id="63d13-177">Microsoft ha desarrollado y publicado varias redes previamente entrenadas que se han usado ampliamente en las tareas de reconocimiento de imágenes, como [ResNet50](https://www.kaggle.com/keras/resnet50).</span><span class="sxs-lookup"><span data-stu-id="63d13-177">Microsoft has developed and published several pre-trained networks that have been widely used for image recognition tasks, such as [ResNet50](https://www.kaggle.com/keras/resnet50).</span></span>

<span data-ttu-id="63d13-178">Dependiendo de la red neuronal, el vector de características será más o menos largo y disperso, de ahí que los requisitos de memoria y almacenamiento varíen.</span><span class="sxs-lookup"><span data-stu-id="63d13-178">Depending on the neural network, the feature vector will be more or less long and sparse, hence the memory and storage requirements will vary.</span></span>

<span data-ttu-id="63d13-179">Además, puede encontrarse con que diferentes redes son aplicables a diferentes categorías, de ahí que una implementación de la búsqueda visual pueda generar en realidad vectores de características de diverso tamaño.</span><span class="sxs-lookup"><span data-stu-id="63d13-179">Also, you may find that different networks are applicable to different categories, hence an implementation of visual search may actually generate feature vectors of varying size.</span></span>

<span data-ttu-id="63d13-180">Las redes neuronales previamente entrenadas son relativamente fáciles de usar pero podrían no ser tan eficaces como un modelo personalizado entrenado en el catálogo de imágenes.</span><span class="sxs-lookup"><span data-stu-id="63d13-180">Pre-trained neural networks are relatively easy to use but may not be as efficient a custom model trained on your image catalog.</span></span> <span data-ttu-id="63d13-181">Estas redes están diseñadas normalmente para la clasificación de conjuntos de datos de referencia y no para realizar búsquedas en una colección de imágenes específica.</span><span class="sxs-lookup"><span data-stu-id="63d13-181">Those pre-trained networks are typically designed for classification of benchmark datasets rather than search on your specific collection of images.</span></span>

<span data-ttu-id="63d13-182">Como es posible que quiera modificarlas y volverlas a entrenar de modo que generen una predicción de categorías y un vector denso (es decir, más pequeño, no disperso), lo que sería muy útil para restringir el espacio de búsqueda, reduzca los requisitos de almacenamiento y memoria.</span><span class="sxs-lookup"><span data-stu-id="63d13-182">You may want to modify and retrain them so they produce both a category prediction and a dense (i.e. smaller, not sparse) vector, which will be very useful to restrict the search space, reduce memory and storage requirements.</span></span> <span data-ttu-id="63d13-183">Se pueden usar vectores binarios y con frecuencia se conocen como "[hash semántico](https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf)": un término derivado de las técnicas de codificación y recuperación de documentos.</span><span class="sxs-lookup"><span data-stu-id="63d13-183">Binary vectors can be used and are often referred to as " [semantic hash](https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf)" – a term derived from document encoding and retrieval techniques.</span></span> <span data-ttu-id="63d13-184">La representación binaria simplifica la realización de cálculos adicionales.</span><span class="sxs-lookup"><span data-stu-id="63d13-184">The binary representation simplifies further calculations.</span></span>

 ![](./assets/visual-search-use-case-overview/resnet-modifications.png)

<span data-ttu-id="63d13-185">*Ilustración 4: Modificaciones de ResNet para Visual Search (F. Yang et al, 2017)*</span><span class="sxs-lookup"><span data-stu-id="63d13-185">*Figure 4: Modifications to ResNet for Visual Search – F. Yang et al., 2017*</span></span>

<span data-ttu-id="63d13-186">Si elige modelos previamente entrenados o prefiere desarrollar los suyos propios, también deberá decidir dónde ejecutar la caracterización o el entrenamiento del modelo propiamente dicho.</span><span class="sxs-lookup"><span data-stu-id="63d13-186">Whether you choose pre-trained models or to develop your own, you will still need to decide where to run the featurization and/or training of the model itself.</span></span>

<span data-ttu-id="63d13-187">Azure ofrece varias opciones: máquinas virtuales, Azure Batch, [Batch AI](https://azure.microsoft.com/services/batch-ai/?WT.mc_id=vsearchgio-article-gmarchet) o clústeres de Databricks.</span><span class="sxs-lookup"><span data-stu-id="63d13-187">Azure offers several options: VMs, Azure Batch, [Batch AI](https://azure.microsoft.com/services/batch-ai/?WT.mc_id=vsearchgio-article-gmarchet), Databricks clusters.</span></span> <span data-ttu-id="63d13-188">Sin embargo, en todos los casos, la mejor relación precio/rendimiento viene dada por el uso de GPU.</span><span class="sxs-lookup"><span data-stu-id="63d13-188">In all cases, however, the best price/performance is given by the use of GPUs.</span></span>

<span data-ttu-id="63d13-189">Microsoft ha anunciado recientemente la disponibilidad de matrices de puertas programables, o FPGA, para el cálculo rápido por una mínima parte del costo de GPU (proyecto [Brainwave](https://www.microsoft.com/research/blog/microsoft-unveils-project-brainwave/?WT.mc_id=vsearchgio-article-gmarchet)).</span><span class="sxs-lookup"><span data-stu-id="63d13-189">Microsoft has also recently announced the availability of FPGAs for fast computation at a fraction of the GPU cost (project [Brainwave](https://www.microsoft.com/research/blog/microsoft-unveils-project-brainwave/?WT.mc_id=vsearchgio-article-gmarchet)).</span></span> <span data-ttu-id="63d13-190">Sin embargo, en el momento de escribir este artículo, esta oferta está limitada a determinadas arquitecturas de red, por lo que deberá evaluar su rendimiento detenidamente.</span><span class="sxs-lookup"><span data-stu-id="63d13-190">However, at the time of writing, this offering is limited to certain network architectures, so you will need to evaluate their performance closely.</span></span>

### <a name="similarity-measure-or-distance"></a><span data-ttu-id="63d13-191">Medida o distancia de similitud</span><span class="sxs-lookup"><span data-stu-id="63d13-191">Similarity Measure or Distance</span></span>

<span data-ttu-id="63d13-192">Cuando las imágenes se representan en el espacio del vector de característica, encontrar similitudes se convierte en una cuestión de definir una medida de distancia entre los puntos de dicho espacio.</span><span class="sxs-lookup"><span data-stu-id="63d13-192">When the images are represented in the feature vector space, finding similarities becomes a question of defining a distance measure between points in such space.</span></span> <span data-ttu-id="63d13-193">Una vez que se ha definido una distancia, puede calcular los clústeres de imágenes similares o definir matrices de similitud.</span><span class="sxs-lookup"><span data-stu-id="63d13-193">Once a distance is defined, you can compute clusters of similar images and/or define similarity matrices.</span></span> <span data-ttu-id="63d13-194">En función de la métrica de distancia seleccionada, los resultados pueden variar.</span><span class="sxs-lookup"><span data-stu-id="63d13-194">Depending on the distance metric selected, the results may vary.</span></span> <span data-ttu-id="63d13-195">La medida de distancia euclidiana más común sobre vectores de números reales, por ejemplo, es fácil de entender: captura la magnitud de la distancia.</span><span class="sxs-lookup"><span data-stu-id="63d13-195">The most common Euclidean distance measure over real-number vectors, for instance, is easy to understand: it captures the magnitude of the distance.</span></span> <span data-ttu-id="63d13-196">Sin embargo, es bastante ineficaz en términos de cálculo.</span><span class="sxs-lookup"><span data-stu-id="63d13-196">However, it is rather inefficient in terms of computation.</span></span>

<span data-ttu-id="63d13-197">La distancia [coseno](https://en.wikipedia.org/wiki/Cosine_similarity) se usa a menudo para capturar la orientación del vector, en lugar de su magnitud.</span><span class="sxs-lookup"><span data-stu-id="63d13-197">[Cosine](https://en.wikipedia.org/wiki/Cosine_similarity) distance is often used to capture the orientation of the vector, rather than its magnitude.</span></span>

<span data-ttu-id="63d13-198">Alternativas como la distancia de [Hamming](https://en.wikipedia.org/wiki/Hamming_distance) a través de representaciones binarias renuncian a cierta precisión a favor de eficiencia y velocidad.</span><span class="sxs-lookup"><span data-stu-id="63d13-198">Alternatives such as [Hamming](https://en.wikipedia.org/wiki/Hamming_distance) distance over binary representations trade some accuracy for efficiency and speed.</span></span>

<span data-ttu-id="63d13-199">La combinación de medida de distancia y tamaño de vector determinará la cantidad de cálculo y memoria que consumirá la búsqueda.</span><span class="sxs-lookup"><span data-stu-id="63d13-199">The combination of vector size and distance measure will determine how computationally intensive and memory intensive the search will be.</span></span>

### <a name="search-amp-ranking"></a><span data-ttu-id="63d13-200">Búsqueda y clasificación</span><span class="sxs-lookup"><span data-stu-id="63d13-200">Search &amp; Ranking</span></span>

<span data-ttu-id="63d13-201">Una vez que se ha definido la similitud, es necesario idear un método eficaz para recuperar los N elementos más cercanos al que se ha pasado como entrada y, luego, devolver una lista de identificadores.</span><span class="sxs-lookup"><span data-stu-id="63d13-201">Once similarity is defined, we need to devise an efficient method to retrieve the closest N items to the one passed as input, then return a list of identifiers.</span></span> <span data-ttu-id="63d13-202">Esto también se conoce como "clasificación de imágenes".</span><span class="sxs-lookup"><span data-stu-id="63d13-202">This is also known as "image ranking".</span></span> <span data-ttu-id="63d13-203">En un conjunto de datos grande, el tiempo para calcular cada distancia es prohibitivo, por lo que se usan algoritmos vecinos aproximados.</span><span class="sxs-lookup"><span data-stu-id="63d13-203">On a large data set, the time to compute every distance is prohibitive, so we use approximate nearest-neighbor algorithms.</span></span> <span data-ttu-id="63d13-204">Para ellos existen varias bibliotecas de código abierto, por lo que no tendrá que escribir el código desde cero.</span><span class="sxs-lookup"><span data-stu-id="63d13-204">Several open source libraries exist for those, so you won't have to code them from scratch.</span></span>

<span data-ttu-id="63d13-205">Por último, los requisitos de memoria y cálculo determinarán la elección de la tecnología de implementación para el modelo entrenado, así como la alta disponibilidad.</span><span class="sxs-lookup"><span data-stu-id="63d13-205">Finally, memory and computation requirements will determine the choice of deployment technology for the trained model, as well high availability.</span></span> <span data-ttu-id="63d13-206">Normalmente, el espacio de búsqueda se particionará y varias instancias del algoritmo de clasificación se ejecutarán en paralelo.</span><span class="sxs-lookup"><span data-stu-id="63d13-206">Typically, the search space will be partitioned, and several instances of the ranking algorithm will run in parallel.</span></span> <span data-ttu-id="63d13-207">Una opción que permite escalabilidad y disponibilidad son los clústeres de [Azure Kubernetes](https://azure.microsoft.com/services/container-service/kubernetes/?WT.mc_id=vsearchgio-article-gmarchet).</span><span class="sxs-lookup"><span data-stu-id="63d13-207">One option that allows for scalability and availability is [Azure Kubernetes](https://azure.microsoft.com/services/container-service/kubernetes/?WT.mc_id=vsearchgio-article-gmarchet) clusters.</span></span> <span data-ttu-id="63d13-208">En ese caso, es conveniente implementar el modelo de clasificación en varios contenedores (de modo que cada uno controle una partición del espacio de búsqueda) y varios nodos (para lograr una alta disponibilidad).</span><span class="sxs-lookup"><span data-stu-id="63d13-208">In that case it is advisable to deploy the ranking model across several containers (handling a partition of the search space each) and several nodes (for high availability).</span></span>

## <a name="next-steps"></a><span data-ttu-id="63d13-209">Pasos siguientes</span><span class="sxs-lookup"><span data-stu-id="63d13-209">Next steps</span></span>

<span data-ttu-id="63d13-210">La implementación de la búsqueda visual no tiene que ser compleja.</span><span class="sxs-lookup"><span data-stu-id="63d13-210">Implementing visual search need not be complex.</span></span> <span data-ttu-id="63d13-211">Puede usar Bing o crear la suya propia con los servicios de Azure, mientras se beneficia de la investigación sobre inteligencia artificial de Microsoft y las herramientas desarrolladas para esta tecnología.</span><span class="sxs-lookup"><span data-stu-id="63d13-211">You can use Bing or build your own with Azure services, while benefiting from Microsoft's AI research and tools.</span></span>

### <a name="trial"></a><span data-ttu-id="63d13-212">Versión de prueba</span><span class="sxs-lookup"><span data-stu-id="63d13-212">Trial</span></span>

- <span data-ttu-id="63d13-213">Pruebe la [consola de prueba de Visual Search API](https://dev.cognitive.microsoft.com/docs/services/878c38e705b84442845e22c7bff8c9ac).</span><span class="sxs-lookup"><span data-stu-id="63d13-213">Try out the [Visual Search API Testing Console](https://dev.cognitive.microsoft.com/docs/services/878c38e705b84442845e22c7bff8c9ac)</span></span>

### <a name="develop"></a><span data-ttu-id="63d13-214">Desarrollo</span><span class="sxs-lookup"><span data-stu-id="63d13-214">Develop</span></span>

- <span data-ttu-id="63d13-215">Para empezar a crear un servicio personalizado, consulte [Introducción a Bing Visual Search API](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/overview/?WT.mc_id=vsearchgio-article-gmarchet).</span><span class="sxs-lookup"><span data-stu-id="63d13-215">To begin creating a customized service, see [Bing Visual Search API Overview](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/overview/?WT.mc_id=vsearchgio-article-gmarchet)</span></span>
- <span data-ttu-id="63d13-216">Para crear su primera solicitud, consulte las guías de inicio rápido: [C#](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/csharp) | [Java](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/java) | [node.js](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/nodejs) | [Python](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/python).</span><span class="sxs-lookup"><span data-stu-id="63d13-216">To create your first request, see the quickstarts: [C#](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/csharp) | [Java](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/java) | [node.js](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/nodejs) | [Python](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/python)</span></span>
- <span data-ttu-id="63d13-217">Familiarícese con la [referencia de Visual Search API](https://aka.ms/bingvisualsearchreferencedoc).</span><span class="sxs-lookup"><span data-stu-id="63d13-217">Familiarize yourself with the [Visual Search API Reference](https://aka.ms/bingvisualsearchreferencedoc).</span></span>

### <a name="background"></a><span data-ttu-id="63d13-218">Fondo</span><span class="sxs-lookup"><span data-stu-id="63d13-218">Background</span></span>

- <span data-ttu-id="63d13-219">[Segmentación de imágenes de aprendizaje profundo](https://www.microsoft.com/developerblog/2018/04/18/deep-learning-image-segmentation-for-ecommerce-catalogue-visual-search/?WT.mc_id=vsearchgio-article-gmarchet): documento de Microsoft que describe el proceso de separar las imágenes de los fondos.</span><span class="sxs-lookup"><span data-stu-id="63d13-219">[Deep Learning Image Segmentation](https://www.microsoft.com/developerblog/2018/04/18/deep-learning-image-segmentation-for-ecommerce-catalogue-visual-search/?WT.mc_id=vsearchgio-article-gmarchet): Microsoft paper describes the process of separating images from backgrounds</span></span>
- <span data-ttu-id="63d13-220">[Visual Search en eBay](https://arxiv.org/abs/1706.03154): investigación de la Universidad de Cornell</span><span class="sxs-lookup"><span data-stu-id="63d13-220">[Visual Search at Ebay](https://arxiv.org/abs/1706.03154): Cornell University research</span></span>
- <span data-ttu-id="63d13-221">[Visual Discovery at Pinterest](https://arxiv.org/abs/1702.04680) (Detección visual en Pinterest): investigación de la Universidad de Cornell.</span><span class="sxs-lookup"><span data-stu-id="63d13-221">[Visual Discovery at Pinterest](https://arxiv.org/abs/1702.04680) Cornell University research</span></span>
- <span data-ttu-id="63d13-222">[Semantic Hashing](https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf) (Hash semántico): investigación de la Universidad de Toronto.</span><span class="sxs-lookup"><span data-stu-id="63d13-222">[Semantic Hashing](https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf) University of Toronto research</span></span>

<span data-ttu-id="63d13-223">_En este artículo fue creado por Giovanni Marchetti y Mariya Zorotovich._</span><span class="sxs-lookup"><span data-stu-id="63d13-223">_This article was authored by Giovanni Marchetti and Mariya Zorotovich._</span></span>